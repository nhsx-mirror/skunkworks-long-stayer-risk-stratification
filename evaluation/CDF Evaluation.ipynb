{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDF Evaluation\n",
    "\n",
    "Evaluate the LTSS risk CDFM using limits of agreement, and LoS stratification (separated by \"long\"/\"short\" as well as by total length of stay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from ltss import los_model, risk_model, vectorise\n",
    "from ltss.utils import reshape_vector, vector_to_dict\n",
    "from training.loader import DataHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate results using LOS and CDF Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LOS_MODEL = los_model.init_model(model_file='../training/mod_ep_110')\n",
    "CDF_MODEL = risk_model.init_model(model_file='../training/trained_shuffle_100_v2.pkl')\n",
    "if os.path.exists('checkpoint.pkl'):\n",
    "    print('Loading Checkpoint')\n",
    "    with open('checkpoint.pkl', 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    print(d.keys(), 'containing', len(d['dataset']), 'entries')\n",
    "    dataset = d['dataset']\n",
    "    true_los = d['true_los']\n",
    "    results = d['results']\n",
    "else:\n",
    "    # Get a DataHandler with the same shuffle and seed settings as used in training\n",
    "    loader = DataHandler('../NHSX Polygeist data 1617 to 2021 v2.csv', shuffle=True, fixed_seed=100, reshape=False)\n",
    "    print(f'Loaded {loader}')\n",
    "    # Use the validation cut of the data only\n",
    "    data, true_los = loader.get_validation()\n",
    "    true_los = true_los.cpu().detach().numpy()\n",
    "    # Flush streams to sync before using tqdm again\n",
    "    sys.stdout.flush()\n",
    "    sys.stderr.flush()\n",
    "    # Get classification for validation data through both models at once\n",
    "    results = []\n",
    "    for i, vector in enumerate(tqdm(data)):\n",
    "        # Convert to dict representation for the risk model, and predict LTSS\n",
    "        vector_dict = vector_to_dict(vector)\n",
    "        forecast = los_model.get_prediction(LOS_MODEL, vector_dict)\n",
    "        risk_predictions = risk_model.get_prediction(CDF_MODEL, vector_dict, \n",
    "                                                     ai_day_prediction=forecast.get('PREDICTED_LOS'))\n",
    "        # Store results\n",
    "        results.append(dict(forecast, **risk_predictions))\n",
    "    dataset = data.cpu().detach().numpy()\n",
    "    # Save checkpoint for fast reuse\n",
    "    with open('checkpoint.pkl', 'wb') as f:\n",
    "        pickle.dump(dict(dataset=dataset, true_los=true_los, results=results), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_risk_score = np.asarray([results[i]['RISK_STRATIFICATION'] for i in range(len(results))])\n",
    "true_risk_score = np.asarray([CDF_MODEL.risk_from_day(x) for x in true_los])\n",
    "\n",
    "print(f'Generated {len(computed_risk_score)} LTSS predictions from {len(true_risk_score)} ground truth entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agreement between True and Predicted LTSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true_los, predicted_los, title, alpha=0.2, cats=None):\n",
    "    irr_x = true_los\n",
    "    irr_y = true_los - predicted_los\n",
    "    mean = np.mean(irr_y)\n",
    "    stdev = np.std(irr_y)\n",
    "    limits = [mean + 1.96 * stdev, mean - 1.96 * stdev]\n",
    "    for confidence in [0.5, 0.75, 0.9, 0.95]:\n",
    "        sds = stats.norm.ppf(1 - (1 - confidence) / 2)\n",
    "        print(f'{title} {int(100 * confidence)}% limits of agreement: Â±{sds*stdev:0.2f}')\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    categories = [[] for i in range(np.max(irr_x))]\n",
    "    for cat, v in zip(irr_x, predicted_los):\n",
    "        categories[cat - 1].append(v)\n",
    "    plt.boxplot(categories)\n",
    "    plt.xlabel('True Risk Score')\n",
    "    plt.ylabel('Predicted Risk Score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate(true_risk_score, computed_risk_score, 'All Validation Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratification of Scores\n",
    "\n",
    "Separated by \"long\" and \"short\" stay, using the 15-day long stay definition discussed in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_stratification(use_los, use_risk_scores, title='Risk Stratification by Stayer Group', long_stay=15):\n",
    "    long_stayers = np.zeros(5)\n",
    "    short_stayers = np.zeros(5)\n",
    "    for i in range(5):\n",
    "        los_in_band = use_los[use_risk_scores == (i + 1)]\n",
    "        count_in_band = len(los_in_band)\n",
    "        if count_in_band > 0:\n",
    "            long_stayers[i] = np.count_nonzero(los_in_band >= long_stay) / count_in_band\n",
    "            short_stayers[i] = np.count_nonzero(los_in_band < long_stay) / count_in_band\n",
    "    print(f'Long stay by band:  {\", \".join([f\"{v:.2f}\" for v in long_stayers])}')\n",
    "    print(f'Short stay by band: {\", \".join([f\"{v:.2f}\" for v in short_stayers])}')\n",
    "\n",
    "    labels = ['Risk 1', 'Risk 2', 'Risk 3', 'Risk 4', 'Risk 5']\n",
    "    width = 0.35  # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(labels, short_stayers, width, label=f'Stayers < {long_stay} Days')\n",
    "    ax.bar(labels, long_stayers, width, bottom=short_stayers, label=f'Long Stayers >= {long_stay} Days')\n",
    "    ax.set_ylabel('Probability of Score')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc='lower left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_stratification(true_los, computed_risk_score, 'All Validation Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratification by banded Length of Stay\n",
    "\n",
    "As above, but instead of separating using \"long\" and \"short\" stay, instead show bands of duration.\n",
    "\n",
    "Plot these with two different spands of duration band, in order to show there isn't an \"end of week\" peak effect skewing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbitrary maximum length of stay (in practice our vectorised LoS estimates are clamped to max 30 days)\n",
    "MAX = 100\n",
    "\n",
    "def evaluate_stratification_banded(use_los, use_risk_scores, title='Risk Stratification by Stayer Group', \n",
    "                                   stays = [7, 14, 21, MAX]):\n",
    "    stayer_bands = [np.zeros(5) for _ in stays]\n",
    "    for i in range(5):\n",
    "        los_in_band = use_los[use_risk_scores == (i + 1)]\n",
    "        count_in_band = len(los_in_band)\n",
    "        if count_in_band > 0:\n",
    "            prev_stay = 0\n",
    "            for los, band in zip(stays, stayer_bands):\n",
    "                curr_proportion = np.count_nonzero(los_in_band < los) / count_in_band\n",
    "                band[i] = curr_proportion - prev_stay\n",
    "                prev_stay = curr_proportion\n",
    "\n",
    "    labels = ['Risk 1', 'Risk 2', 'Risk 3', 'Risk 4', 'Risk 5']\n",
    "    width = 0.35  # the width of the bars: can also be len(x) sequence\n",
    "    print('\\n'.join([f'{l}: ' + ', '.join([f'{v:.2f}' for v in a]) for l, a in zip(stays, stayer_bands)]))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    prev_band = None\n",
    "    prev_los = None\n",
    "    for los, band in zip(stays, stayer_bands):\n",
    "        if prev_band is None:\n",
    "            ax.bar(labels, band, width, label=f'Stay < {los} Days')\n",
    "            prev_band = band\n",
    "        else:\n",
    "            ax.bar(labels, band, width, bottom=prev_band, \n",
    "                   label=f'{prev_los} <= Stay < {los} Days' if los < MAX else f'Stay >= {prev_los} Days')\n",
    "            prev_band += band\n",
    "        prev_los = los\n",
    "    ax.set_ylabel('Probability of Length of Stay in Band')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc='lower left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_stratification_banded(true_los, computed_risk_score, 'All Validation Data')\n",
    "evaluate_stratification_banded(true_los, computed_risk_score, 'All Validation Data', stays=[5, 10, 15, 20, 25, MAX])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
